{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8277daf3-e0d9-4efa-9058-4b4e67d41572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import miditok\n",
    "from miditok import REMI, TokenizerConfig\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from symusic import Score\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ded318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdae2af",
   "metadata": {},
   "source": [
    "![dataset](images/Dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54e8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrangerEmbedding(nn.Module):\n",
    "  def __init__(self, arranger_ids=256, hidden_size=128):\n",
    "    super().__init__()\n",
    "    self.embeddings = nn.Embedding(arranger_ids, hidden_size)\n",
    "\n",
    "  def forward(self, arranger_id, mel_db):\n",
    "    return torch.cat([self.embeddings(arranger_id), mel_db], dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3574a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tokenizer, sample_rate=22050, music_folder=\"song2midi/dataset/song\", midi_folder=\"song2midi/dataset/midi\", output_folder=\"song2midi/dataset/preprocess\"):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_folder, \"mel\")):\n",
    "        os.makedirs(os.path.join(output_folder, \"mel\"))\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_folder, \"midi\")):\n",
    "        os.makedirs(os.path.join(output_folder, \"midi\"))\n",
    "                    \n",
    "    music_ids = [name.split(\".\")[0] for name in os.listdir(music_folder)]\n",
    "    uncheck_midis = [name.split(\".\")[:2] for name in os.listdir(midi_folder)]\n",
    "    midis = [midi for midi in uncheck_midis if midi[0] in music_ids] # [music_id, arranger_id]\n",
    "\n",
    "    skipped_music = []\n",
    "\n",
    "    for music_id in music_ids:\n",
    "        music_path = os.path.join(music_folder, f\"{music_id}.mp3\")\n",
    "        waveform, sr = torchaudio.load(music_path)\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=4096, hop_length=1024, n_mels=128)\n",
    "        mel = mel_transform(waveform)\n",
    "        mel_db = torchaudio.transforms.AmplitudeToDB()(mel)\n",
    "\n",
    "        mel_shape = mel_db.shape\n",
    "        if mel_shape[2] > 6144: # have to skip this music\n",
    "            print(f\"music id: {music_id} is too long skipping\")\n",
    "            skipped_music.append(music_id)\n",
    "            continue\n",
    "\n",
    "        mel_db = mel_db.reshape(mel_shape[0], mel_shape[2], mel_shape[1])\n",
    "\n",
    "        np.save(os.path.join(output_folder, \"mel\", f\"{music_id}.mel.npy\"), mel_db.numpy())\n",
    "\n",
    "    for music_id, arranger_id in midis:\n",
    "        if music_id in skipped_music:\n",
    "            continue\n",
    "\n",
    "        midi_path = os.path.join(midi_folder, f\"{music_id}.{arranger_id}.mid\")\n",
    "        score = Score(midi_path)\n",
    "        miditok.utils.merge_same_program_tracks(score.tracks)\n",
    "        midi_encoded = tokenizer.encode(score)\n",
    "        np.save(os.path.join(output_folder, \"midi\", f\"{music_id}.{arranger_id}.midi.npy\"), midi_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a46acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_interpolate(mel):\n",
    "    mel_shape = mel.shape[1]\n",
    "    return nn.functional.interpolate(mel.unsqueeze(1), size=(math.floor(mel_shape / 4), 128), mode='bilinear', align_corners=False).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674b8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, tokenizer, embbeding, max_output_length=4096, max_input_length=6144, mel_processing=None, preprocess_folder=\"song2midi/dataset/preprocess\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embbeding = embbeding\n",
    "        self.max_output_length = max_output_length\n",
    "        self.max_input_length = max_input_length\n",
    "        self.preprocess_folder = preprocess_folder\n",
    "        self.mel_processing = mel_processing\n",
    "        \n",
    "        self.data_files = [name.split(\".\")[:2] for name in os.listdir(os.path.join(preprocess_folder, \"midi\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        music_id, arranger_id = self.data_files[idx]\n",
    "\n",
    "        # load precomputed mel'\n",
    "        mel_db = np.load(os.path.join(self.preprocess_folder, \"mel\", f\"{music_id}.mel.npy\"))\n",
    "        mel_db = torch.tensor(mel_db)\n",
    "\n",
    "        if self.mel_processing:\n",
    "            mel_db = self.mel_processing(mel_db)\n",
    "\n",
    "        if mel_db.shape[1] < self.max_input_length - 1:\n",
    "            num_pad = self.max_input_length - mel_db.shape[1] - 1\n",
    "            mel_padded = torch.cat([mel_db, torch.zeros((1, num_pad, mel_db.shape[2]))], dim=1)\n",
    "        else:\n",
    "            num_pad = 0\n",
    "            mel_padded = mel_db[:, :self.max_input_length - 1]\n",
    "\n",
    "        input_embed = self.embbeding(torch.tensor([[int(arranger_id)]]), mel_padded)\n",
    "        attention_mask = torch.cat([torch.ones(mel_db.shape[:2], dtype=torch.int32), torch.zeros((mel_db.shape[0], num_pad + 1))], dim=1)\n",
    "\n",
    "        global_attention_mask = torch.zeros(input_embed.shape[:2], dtype=torch.int32)\n",
    "        global_attention_mask[:, 0] = 1\n",
    "\n",
    "        # load precomputed midi\n",
    "        midi_encoded = np.load(os.path.join(self.preprocess_folder, \"midi\", f\"{music_id}.{arranger_id}.midi.npy\"))\n",
    "        midi = torch.tensor(midi_encoded, dtype=torch.int32)\n",
    "        midi = torch.cat([torch.tensor([[self.tokenizer[\"BOS_None\"]]]), midi, torch.tensor([[self.tokenizer[\"EOS_None\"]]])], dim=-1)\n",
    "\n",
    "        # padding with -100\n",
    "        if midi.shape[1] < self.max_output_length:\n",
    "            midi = torch.cat([midi, torch.ones((midi.shape[0], self.max_output_length - midi.shape[1]), dtype=torch.int32) * -100], dim=-1)\n",
    "        else:\n",
    "            midi = midi[:, :self.max_output_length]\n",
    "\n",
    "        return {\"inputs_embeds\": input_embed.squeeze(0), \n",
    "                \"attention_mask\": attention_mask.squeeze(0), \n",
    "                #\"global_attention_mask\": global_attention_mask.squeeze(0), \n",
    "                \"labels\": midi.squeeze(0)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522d1ae",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5bf2bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34917/2731034332.py:14: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  remi_config = TokenizerConfig(**TOKENIZER_PARAMS)\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": (0, 127),\n",
    "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "    \"num_velocities\": 32,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "    \"use_chords\": True,\n",
    "    \"use_rests\": False,\n",
    "    \"use_tempos\": True,\n",
    "    \"use_time_signatures\": False,\n",
    "    \"use_programs\": False,\n",
    "    \"num_tempos\": 32,  # number of tempo bins\n",
    "    \"tempo_range\": (40, 250),  # (min, max)\n",
    "}\n",
    "remi_config = TokenizerConfig(**TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4af7d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_tokenizer = REMI(remi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a824404",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = REMI(remi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48423d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/miditok/midi_tokenizer.py:3252: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  self.config = TokenizerConfig()\n",
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/miditok/classes.py:702: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  return cls(**input_dict, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = REMI(params=\"song2midi/dataset/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6751f1a-0022-49ac-b688-c178f53d7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_paths = list(Path(\"song2midi\", \"dataset\", \"midi_blend\").glob(\"*.mid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa7fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(vocab_size=50265, files_paths=midi_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e76718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_params(\"song2midi/dataset/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552bd5e",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81e8dd1-ce38-4584-9060-74cc10ee6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music id: -7AMMFFiGLU is too long skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music id: 7Y-XlFtao_A is too long skipping\n",
      "music id: bRBn5EY505E is too long skipping\n",
      "music id: C5tXGwUCD1c is too long skipping\n",
      "music id: cBC6c99OrOM is too long skipping\n",
      "music id: CPLK6L1fq7k is too long skipping\n",
      "music id: f3YhK-oZmY8 is too long skipping\n",
      "music id: gwOGlPU63Dw is too long skipping\n",
      "music id: HKtxkk3JvbQ is too long skipping\n",
      "music id: hTbeVXuWyaU is too long skipping\n",
      "music id: k0J1z82kVYQ is too long skipping\n",
      "music id: KpS-9OcYyqk is too long skipping\n",
      "music id: LEJlWRpqy2U is too long skipping\n",
      "music id: lzyl7abaPe0 is too long skipping\n",
      "music id: nFG3l5zxLdM is too long skipping\n",
      "music id: Pa4EGtUfDvI is too long skipping\n",
      "music id: r1k3hpTdc60 is too long skipping\n",
      "music id: RcQy61-7F7c is too long skipping\n",
      "music id: S8HAUbBLCPM is too long skipping\n",
      "music id: T4yW6TwjNLM is too long skipping\n",
      "music id: VnWo9-Dioik is too long skipping\n",
      "music id: XbZHt8b7YTY is too long skipping\n",
      "music id: XKROtTZs8iE is too long skipping\n",
      "music id: Y0ygWXF-5tI is too long skipping\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(tokenizer, music_folder=\"song2midi/dataset/song_blend\", midi_folder=\"song2midi/dataset/midi_blend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c81db",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac148cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicDataset(tokenizer, ArrangerEmbedding(), max_output_length=4096, max_input_length=6144, mel_processing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eee934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in dataset:\n",
    "    if a[\"inputs_embeds\"].shape[0] != 6144 or a[\"attention_mask\"].shape[0] != 6144 or a[\"labels\"].shape[0] != 4096:\n",
    "        print(a[\"inputs_embeds\"].shape, a[\"attention_mask\"].shape, a[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5adbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3328b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b1d91",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b99489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da17ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reformer_pytorch import ReformerLM, Reformer\n",
    "from axial_positional_embedding import AxialPositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c386d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerEncoderDecoderConfig(PretrainedConfig):\n",
    "    def __init__(self,\n",
    "                  vocab_size=tokenizer.vocab_size, \n",
    "                  d_model=128,\n",
    "                  num_heads=8, \n",
    "                  encoder_layers=6, \n",
    "                  decoder_layers=6, \n",
    "                  encoder_max_seq_len=6144,\n",
    "                  decoder_max_seq_len=4096,\n",
    "                  encoder_axial_position_shape=(96, 64),\n",
    "                  decoder_axial_position_shape=(64, 64),\n",
    "                  pad_token_id=0,\n",
    "                  bos_token_id=1,\n",
    "                  eos_token_id=2,\n",
    "                  **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.encoder_max_seq_len = encoder_max_seq_len\n",
    "        self.decoder_max_seq_len = decoder_max_seq_len\n",
    "        self.encoder_axial_position_shape = encoder_axial_position_shape\n",
    "        self.decoder_axial_position_shape = decoder_axial_position_shape\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53617dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerEncoderDecoder(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "        self.bos_token_id = config.bos_token_id\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "\n",
    "        self.encoder = Reformer(\n",
    "            dim=config.d_model,\n",
    "            depth=config.encoder_layers,\n",
    "            heads=config.num_heads,\n",
    "        )\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim=config.d_model,\n",
    "            depth=config.decoder_layers,\n",
    "            heads=config.num_heads,\n",
    "            max_seq_len=config.decoder_max_seq_len,\n",
    "            num_tokens=config.vocab_size,\n",
    "            axial_position_emb=True,\n",
    "            axial_position_shape=config.decoder_axial_position_shape,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        self.position_embedding = AxialPositionalEmbedding(\n",
    "            config.d_model,\n",
    "            axial_shape=config.encoder_axial_position_shape\n",
    "        )\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    def pad_to_multiple(self, tensor, seq_len, multiple, dim=-1):\n",
    "        m = seq_len / multiple\n",
    "        if m.is_integer():\n",
    "            return tensor\n",
    "        \n",
    "        remainder = math.ceil(m) * multiple - seq_len\n",
    "        pad_offset = (0,) * (-1 - dim) * 2\n",
    "        return nn.functional.pad(tensor, (*pad_offset, 0, remainder), value=self.pad_token_id)\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    # pad_dim = -1 if its LM model else -2\n",
    "    def auto_paddding(self, input_ids, pad_dim, bucket_size, num_mem_kv, full_attn_thres, keys=None, input_mask=None, input_attn_mask=None):\n",
    "        device = input_ids.device\n",
    "\n",
    "        batch_size, t = input_ids.shape[:2]\n",
    "\n",
    "        keys_len = 0 if keys is None else keys.shape[1]\n",
    "        seq_len = t + num_mem_kv + keys_len\n",
    "        \n",
    "\n",
    "        if seq_len > full_attn_thres:\n",
    "            if input_mask is None:\n",
    "                input_mask = torch.full((batch_size, t), True, dtype=torch.bool, device=device)\n",
    "\n",
    "            input_ids = self.pad_to_multiple(input_ids, seq_len, bucket_size * 2, dim=pad_dim)\n",
    "\n",
    "            if input_mask is not None:\n",
    "                input_mask = nn.functional.pad(input_mask, (0, input_ids.shape[1] - input_mask.shape[1]), value=False)\n",
    "\n",
    "            if input_attn_mask is not None:\n",
    "                offset = input_ids.shape[1] - input_attn_mask.shape[1]\n",
    "                input_attn_mask = nn.functional.pad(input_attn_mask, (0, offset, 0, offset), value=False)\n",
    "\n",
    "        return input_ids, input_mask, input_attn_mask\n",
    "\n",
    "\n",
    "    def shift_tokens_right(self, input_ids):\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = self.eos_token_id\n",
    "\n",
    "        if self.pad_token_id is None:\n",
    "            raise ValueError(\"config.pad_token_id has to be defined.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, self.pad_token_id)\n",
    "\n",
    "        return shifted_input_ids\n",
    "\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None, decoder_input=None, labels=None):\n",
    "        if decoder_input is None:\n",
    "            decoder_input = self.shift_tokens_right(labels)\n",
    "\n",
    "        # encoder\n",
    "        encoder_input = inputs_embeds + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_output = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        decoder_output = self.decoder(decoder_input, keys=encoder_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(decoder_output.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            return {\"loss\": masked_lm_loss, \"logits\": decoder_output}\n",
    "        \n",
    "        return {\"logits\": decoder_output}\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, inputs_embeds, attention_mask=None, max_length=4096, temperature=1.0, top_k=50, top_p=1):\n",
    "        is_training = self.training\n",
    "        device = inputs_embeds.device\n",
    "\n",
    "        # padding settings\n",
    "        pad_dim = -1\n",
    "        bucket_size = self.decoder.reformer.bucket_size\n",
    "        num_mem_kv = self.decoder.reformer.num_mem_kv\n",
    "        full_attn_thres = self.decoder.reformer.full_attn_thres\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        # encoder\n",
    "        encoder_input = inputs_embeds + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_keys = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        generated = torch.tensor([self.bos_token_id], device=device).unsqueeze(0)\n",
    "\n",
    "        decoder_mask = torch.full_like(generated, True, dtype=torch.bool, device=device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            generated = generated[:, -self.config.decoder_max_seq_len:]\n",
    "            decoder_mask = decoder_mask[:, -self.config.decoder_max_seq_len:]\n",
    "\n",
    "            generated, decoder_mask, _ = self.auto_paddding(generated, \n",
    "                                                             pad_dim, \n",
    "                                                             bucket_size, \n",
    "                                                             num_mem_kv, \n",
    "                                                             full_attn_thres, \n",
    "                                                             keys=encoder_keys, \n",
    "                                                             input_mask=decoder_mask)\n",
    "            \n",
    "            logits = self.decoder(generated, input_mask=decoder_mask, keys=encoder_keys)[:, -1, :]  / temperature\n",
    "\n",
    "            if top_k > 0:\n",
    "                top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "                filtered_logits = torch.full_like(logits, -float('Inf'))\n",
    "                logits = filtered_logits.scatter(1, top_k_indices, top_k_values)\n",
    "\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "                sorted_logits[sorted_indices_to_remove] = -float('Inf')\n",
    "                logits = sorted_logits.scatter(1, sorted_indices, sorted_logits)\n",
    "\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "            if next_token == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "        self.train(is_training)\n",
    "        return generated\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a0098",
   "metadata": {},
   "source": [
    "## Absolute pos No use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8edbf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device)\n",
    "        return self.emb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59be388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerEncoderDecoder(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "        self.bos_token_id = config.bos_token_id\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "\n",
    "        self.projection = nn.Linear(128, config.d_model, bias=False)\n",
    "\n",
    "        self.encoder = Reformer(\n",
    "            dim=config.d_model,\n",
    "            depth=config.encoder_layers,\n",
    "            heads=config.num_heads,\n",
    "        )\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim=config.d_model,\n",
    "            depth=config.decoder_layers,\n",
    "            heads=config.num_heads,\n",
    "            max_seq_len=config.decoder_max_seq_len,\n",
    "            num_tokens=config.vocab_size,\n",
    "            absolute_position_emb=True,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        self.position_embedding = AbsolutePositionalEmbedding(\n",
    "            config.d_model,\n",
    "            config.encoder_max_seq_len\n",
    "        )\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    def pad_to_multiple(self, tensor, seq_len, multiple, dim=-1):\n",
    "        m = seq_len / multiple\n",
    "        if m.is_integer():\n",
    "            return tensor\n",
    "        \n",
    "        remainder = math.ceil(m) * multiple - seq_len\n",
    "        pad_offset = (0,) * (-1 - dim) * 2\n",
    "        return nn.functional.pad(tensor, (*pad_offset, 0, remainder), value=self.pad_token_id)\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    # pad_dim = -1 if its LM model else -2\n",
    "    def auto_paddding(self, input_ids, pad_dim, bucket_size, num_mem_kv, full_attn_thres, keys=None, input_mask=None, input_attn_mask=None):\n",
    "        device = input_ids.device\n",
    "\n",
    "        batch_size, t = input_ids.shape[:2]\n",
    "\n",
    "        keys_len = 0 if keys is None else keys.shape[1]\n",
    "        seq_len = t + num_mem_kv + keys_len\n",
    "        \n",
    "\n",
    "        if seq_len > full_attn_thres:\n",
    "            if input_mask is None:\n",
    "                input_mask = torch.full((batch_size, t), True, dtype=torch.bool, device=device)\n",
    "\n",
    "            input_ids = self.pad_to_multiple(input_ids, seq_len, bucket_size * 2, dim=pad_dim)\n",
    "\n",
    "            if input_mask is not None:\n",
    "                input_mask = nn.functional.pad(input_mask, (0, input_ids.shape[1] - input_mask.shape[1]), value=False)\n",
    "\n",
    "            if input_attn_mask is not None:\n",
    "                offset = input_ids.shape[1] - input_attn_mask.shape[1]\n",
    "                input_attn_mask = nn.functional.pad(input_attn_mask, (0, offset, 0, offset), value=False)\n",
    "\n",
    "        return input_ids, input_mask, input_attn_mask\n",
    "\n",
    "\n",
    "    def shift_tokens_right(self, input_ids):\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = self.eos_token_id\n",
    "\n",
    "        if self.pad_token_id is None:\n",
    "            raise ValueError(\"config.pad_token_id has to be defined.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, self.pad_token_id)\n",
    "\n",
    "        return shifted_input_ids\n",
    "\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None, decoder_input=None, labels=None):\n",
    "        if decoder_input is None:\n",
    "            decoder_input = self.shift_tokens_right(labels)\n",
    "\n",
    "        # encoder\n",
    "        projected_input = self.projection(inputs_embeds)\n",
    "\n",
    "        encoder_input = projected_input + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_output = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        decoder_output = self.decoder(decoder_input, keys=encoder_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(decoder_output.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            return {\"loss\": masked_lm_loss, \"logits\": decoder_output}\n",
    "        \n",
    "        return {\"logits\": decoder_output}\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, inputs_embeds, attention_mask=None, max_length=4096, temperature=1.0, top_k=50, top_p=1):\n",
    "        is_training = self.training\n",
    "        device = inputs_embeds.device\n",
    "\n",
    "        # padding settings\n",
    "        pad_dim = -1\n",
    "        bucket_size = self.decoder.reformer.bucket_size\n",
    "        num_mem_kv = self.decoder.reformer.num_mem_kv\n",
    "        full_attn_thres = self.decoder.reformer.full_attn_thres\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        # encoder\n",
    "        projected_input = self.projection(inputs_embeds)\n",
    "\n",
    "        encoder_input = projected_input + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_keys = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        generated = torch.tensor([self.bos_token_id], device=device).unsqueeze(0)\n",
    "\n",
    "        decoder_mask = torch.full_like(generated, True, dtype=torch.bool, device=device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            generated = generated[:, -self.config.decoder_max_seq_len:]\n",
    "            decoder_mask = decoder_mask[:, -self.config.decoder_max_seq_len:]\n",
    "\n",
    "            generated, decoder_mask, _ = self.auto_paddding(generated, \n",
    "                                                             pad_dim, \n",
    "                                                             bucket_size, \n",
    "                                                             num_mem_kv, \n",
    "                                                             full_attn_thres, \n",
    "                                                             keys=encoder_keys, \n",
    "                                                             input_mask=decoder_mask)\n",
    "            \n",
    "            logits = self.decoder(generated, input_mask=decoder_mask, keys=encoder_keys)[:, -1, :]  / temperature\n",
    "\n",
    "            if top_k > 0:\n",
    "                top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "                filtered_logits = torch.full_like(logits, -float('Inf'))\n",
    "                logits = filtered_logits.scatter(1, top_k_indices, top_k_values)\n",
    "\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "                sorted_logits[sorted_indices_to_remove] = -float('Inf')\n",
    "                logits = sorted_logits.scatter(1, sorted_indices, sorted_logits)\n",
    "\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "            if next_token == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "        self.train(is_training)\n",
    "        return generated\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348619f",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2155a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RedConfig = ReformerEncoderDecoderConfig()\n",
    "\n",
    "model = ReformerEncoderDecoder(RedConfig).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90165c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m model(inputs_embeds\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6144\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6144\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50265\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4096\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "a = model(inputs_embeds=torch.rand((1, 6144, 128)).cuda(), attention_mask=torch.zeros(1, 6144).bool().cuda(), labels=torch.randint(0, 50265, (1, 4096)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b041ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"song2midi/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c69623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71ab84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncoderDecoder(\n",
       "  (encoder): Reformer(\n",
       "    (layers): ReversibleSequence(\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): LSHSelfAttention(\n",
       "                (toqk): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (tov): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): FullQKAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (local_attn): LocalAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Chunk(\n",
       "                (fn): FeedForward(\n",
       "                  (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (irrev_blocks): ModuleList(\n",
       "        (0-5): 6 x IrreversibleBlock(\n",
       "          (f): PreNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): LSHSelfAttention(\n",
       "              (toqk): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (tov): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (lsh_attn): LSHAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (full_attn): FullQKAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (local_attn): LocalAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (g): PreNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ReformerLM(\n",
       "    (token_emb): Embedding(50265, 128)\n",
       "    (to_model_dim): Identity()\n",
       "    (pos_emb): AxialPositionalEmbedding()\n",
       "    (layer_pos_emb): Always()\n",
       "    (reformer): Reformer(\n",
       "      (layers): ReversibleSequence(\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x ReversibleBlock(\n",
       "            (f): Deterministic(\n",
       "              (net): PreNorm(\n",
       "                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=128, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=128, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (g): Deterministic(\n",
       "              (net): PreNorm(\n",
       "                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (irrev_blocks): ModuleList(\n",
       "          (0-5): 6 x IrreversibleBlock(\n",
       "            (f): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): LSHSelfAttention(\n",
       "                (toqk): Linear(in_features=128, out_features=512, bias=False)\n",
       "                (tov): Linear(in_features=128, out_features=512, bias=False)\n",
       "                (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): FullQKAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (local_attn): LocalAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (g): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Chunk(\n",
       "                (fn): FeedForward(\n",
       "                  (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (out): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Linear(in_features=128, out_features=50265, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (position_embedding): AxialPositionalEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6354e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 1 loss: 3.001537322998047: 100%|████████████| 200/200 [13:25<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1 avg_loss: 3.292593777179718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 loss: 2.7438926696777344 tok_error: 0.1187744140625: 100%|█| 40/40 [00:21<00:00,  1.87it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 avg_loss: 3.214534121751785 avg_tok_error: 0.136553955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 2 loss: 3.1599314212799072: 100%|███████████| 200/200 [13:14<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 2 avg_loss: 3.263686016201973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 2 loss: 2.758176326751709 tok_error: 0.09735107421875: 100%|█| 40/40 [00:21<00:00,  1.90it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 2 avg_loss: 3.206604093313217 avg_tok_error: 0.116510009765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 3 loss: 3.634474992752075: 100%|████████████| 200/200 [13:13<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 3 avg_loss: 3.2527723687887193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 3 loss: 2.7613515853881836 tok_error: 0.114013671875: 100%|█| 40/40 [00:21<00:00,  1.89it/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 3 avg_loss: 3.1994731426239014 avg_tok_error: 0.13108978271484376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 4 loss: 3.4382355213165283: 100%|███████████| 200/200 [12:39<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 4 avg_loss: 3.251042654514313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 4 loss: 2.770627975463867 tok_error: 0.1146240234375: 100%|█| 40/40 [00:20<00:00,  1.98it/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 4 avg_loss: 3.1884994506835938 avg_tok_error: 0.13695068359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 5 loss: 2.9310972690582275: 100%|███████████| 200/200 [12:38<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 5 avg_loss: 3.2381991171836852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 5 loss: 2.7556653022766113 tok_error: 0.1142578125: 100%|█| 40/40 [00:20<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 5 avg_loss: 3.179256671667099 avg_tok_error: 0.1314971923828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 6 loss: 3.547208786010742:  94%|███████████▎| 189/200 [11:57<00:41,  3.79s/it]"
     ]
    }
   ],
   "source": [
    "dataloader_train = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "num_epochs = 100\n",
    " \n",
    "optimizer = AdamW(model.parameters(), lr=0.0003125)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=5, factor=0.3125, threshold=0.01)\n",
    "\n",
    "old_loss = 1000000\n",
    "time = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # training\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    progess_bar = tqdm(dataloader_train)\n",
    "    for batch in progess_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outp = model(\n",
    "            inputs_embeds=batch[\"inputs_embeds\"].cuda(),\n",
    "            attention_mask=batch['attention_mask'].to('cuda'),\n",
    "            labels=batch[\"labels\"].cuda()\n",
    "            )\n",
    "        \n",
    "        loss = outp[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        progess_bar.set_description(f\"lr: {scheduler.get_last_lr()[0]} training epoch: {epoch+1} loss: {loss.item()}\")\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(dataloader_train)\n",
    "    print(f\"training epoch: {epoch+1} avg_loss: {avg_train_loss}\")\n",
    "\n",
    "    # evaluation\n",
    "    val_loss = 0\n",
    "    tok_error = 0\n",
    "    model.eval()\n",
    "    progess_bar = tqdm(dataloader_val)\n",
    "    for batch in progess_bar:\n",
    "        with torch.no_grad():\n",
    "            outp = model(\n",
    "                inputs_embeds=batch[\"inputs_embeds\"].cuda(),\n",
    "                attention_mask=batch['attention_mask'].to('cuda'),\n",
    "                labels=batch[\"labels\"].cuda()\n",
    "                )\n",
    "            \n",
    "            loss = outp[\"loss\"]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            tok_err = torch.mean(torch.tensor(tokenizer.tokens_errors(torch.argmax(outp[\"logits\"], dim=-1).cpu()))).item()\n",
    "            tok_error += tok_err\n",
    "\n",
    "            progess_bar.set_description(f\"validation epoch: {epoch+1} loss: {loss.item()} tok_error: {tok_err}\")\n",
    "\n",
    "    avg_val_loss = val_loss / len(dataloader_val)\n",
    "    avg_tok_error = tok_error / len(dataloader_val)\n",
    "    print(f\"validation epoch: {epoch+1} avg_loss: {avg_val_loss} avg_tok_error: {avg_tok_error}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < old_loss:\n",
    "        old_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"song2midi/check/reformer_encoder_decoder_{time}_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb0fc2",
   "metadata": {},
   "source": [
    "# Compute Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "066ba57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c00d50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_no_padding(labels, logits, pad_index=-100):\n",
    "    avg_score = 0\n",
    "    num = 0\n",
    "    for label, logit in zip(labels, logits):\n",
    "        #output = logit.argmax(dim=-1)\n",
    "        output = logit\n",
    "\n",
    "        valid = label != pad_index\n",
    "        bpe_label = label[valid].unsqueeze(0).numpy()\n",
    "        bpe_output = output[valid].unsqueeze(0).numpy()\n",
    "\n",
    "        #print(np.array(plain_tokenizer.encode(tokenizer.decode(bpe_label))).tolist()[0])\n",
    "        no_bpe_label = \" \".join([str(num) for num in np.array(plain_tokenizer.encode(tokenizer.decode(bpe_label))).tolist()[0]])\n",
    "        no_bpe_output = \" \".join([str(num) for num in np.array(plain_tokenizer.encode(tokenizer.decode(bpe_output))).tolist()[0]])\n",
    "\n",
    "        avg_score += torcheval.metrics.functional.bleu_score(no_bpe_output, [no_bpe_label]).item()\n",
    "        num += 1\n",
    "\n",
    "    return avg_score / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0fc8a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_bleu_score 0.10628176755271852\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity between two labels\n",
    "\n",
    "avg_acc = 0\n",
    "for batch in DataLoader(val_dataset, batch_size=4, shuffle=False):\n",
    "    with torch.no_grad():\n",
    "        outp = model(\n",
    "            inputs_embeds=batch[\"inputs_embeds\"].cuda(),\n",
    "            attention_mask=batch['attention_mask'].to('cuda'),\n",
    "            labels=batch[\"labels\"].cuda()\n",
    "            )\n",
    "\n",
    "        avg_acc += bleu_no_padding(batch[\"labels\"], torch.argmax(outp[\"logits\"], dim=-1).cpu())\n",
    "\n",
    "\n",
    "print(\"avg_bleu_score\", avg_acc / len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bae054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
