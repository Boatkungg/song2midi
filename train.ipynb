{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8277daf3-e0d9-4efa-9058-4b4e67d41572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import miditok\n",
    "from miditok import REMI, TokenizerConfig\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from symusic import Score\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ba3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ded318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdae2af",
   "metadata": {},
   "source": [
    "![dataset](images/Dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54e8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrangerEmbedding(nn.Module):\n",
    "  def __init__(self, arranger_ids=256, hidden_size=128):\n",
    "    super().__init__()\n",
    "    self.embeddings = nn.Embedding(arranger_ids, hidden_size)\n",
    "\n",
    "  def forward(self, arranger_id, mel_db):\n",
    "    return torch.cat([self.embeddings(arranger_id), mel_db], dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3574a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tokenizer, sample_rate=22050, music_folder=\"song2midi/dataset/song\", midi_folder=\"song2midi/dataset/midi\", output_folder=\"song2midi/dataset/preprocess\"):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_folder, \"mel\")):\n",
    "        os.makedirs(os.path.join(output_folder, \"mel\"))\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_folder, \"midi\")):\n",
    "        os.makedirs(os.path.join(output_folder, \"midi\"))\n",
    "                    \n",
    "    music_ids = [name.split(\".\")[0] for name in os.listdir(music_folder)]\n",
    "    uncheck_midis = [name.split(\".\")[:2] for name in os.listdir(midi_folder)]\n",
    "    midis = [midi for midi in uncheck_midis if midi[0] in music_ids] # [music_id, arranger_id]\n",
    "\n",
    "    skipped_music = []\n",
    "\n",
    "    for music_id in music_ids:\n",
    "        music_path = os.path.join(music_folder, f\"{music_id}.mp3\")\n",
    "        waveform, sr = torchaudio.load(music_path)\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=4096, hop_length=1024, n_mels=128)\n",
    "        mel = mel_transform(waveform)\n",
    "        mel_db = torchaudio.transforms.AmplitudeToDB()(mel)\n",
    "\n",
    "        mel_shape = mel_db.shape\n",
    "        if mel_shape[2] > 6144: # have to skip this music\n",
    "            print(f\"music id: {music_id} is too long skipping\")\n",
    "            skipped_music.append(music_id)\n",
    "            continue\n",
    "\n",
    "        mel_db = mel_db.reshape(mel_shape[0], mel_shape[2], mel_shape[1])\n",
    "\n",
    "        np.save(os.path.join(output_folder, \"mel\", f\"{music_id}.mel.npy\"), mel_db.numpy())\n",
    "\n",
    "    for music_id, arranger_id in midis:\n",
    "        if music_id in skipped_music:\n",
    "            continue\n",
    "\n",
    "        midi_path = os.path.join(midi_folder, f\"{music_id}.{arranger_id}.mid\")\n",
    "        score = Score(midi_path)\n",
    "        miditok.utils.merge_same_program_tracks(score.tracks)\n",
    "        midi_encoded = tokenizer.encode(score)\n",
    "        np.save(os.path.join(output_folder, \"midi\", f\"{music_id}.{arranger_id}.midi.npy\"), midi_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a46acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_interpolate(mel):\n",
    "    mel_shape = mel.shape[1]\n",
    "    return nn.functional.interpolate(mel.unsqueeze(1), size=(math.floor(mel_shape / 4), 128), mode='bilinear', align_corners=False).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674b8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, tokenizer, embbeding, max_output_length=4096, max_input_length=6144, mel_processing=None, preprocess_folder=\"song2midi/dataset/preprocess\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embbeding = embbeding\n",
    "        self.max_output_length = max_output_length\n",
    "        self.max_input_length = max_input_length\n",
    "        self.preprocess_folder = preprocess_folder\n",
    "        self.mel_processing = mel_processing\n",
    "        \n",
    "        self.data_files = [name.split(\".\")[:2] for name in os.listdir(os.path.join(preprocess_folder, \"midi\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        music_id, arranger_id = self.data_files[idx]\n",
    "\n",
    "        # load precomputed mel'\n",
    "        mel_db = np.load(os.path.join(self.preprocess_folder, \"mel\", f\"{music_id}.mel.npy\"))\n",
    "        mel_db = torch.tensor(mel_db)\n",
    "\n",
    "        if self.mel_processing:\n",
    "            mel_db = self.mel_processing(mel_db)\n",
    "\n",
    "        if mel_db.shape[1] < self.max_input_length - 1:\n",
    "            num_pad = self.max_input_length - mel_db.shape[1] - 1\n",
    "            mel_padded = torch.cat([mel_db, torch.zeros((1, num_pad, mel_db.shape[2]))], dim=1)\n",
    "        else:\n",
    "            num_pad = 0\n",
    "            mel_padded = mel_db[:, :self.max_input_length - 1]\n",
    "\n",
    "        input_embed = self.embbeding(torch.tensor([[int(arranger_id)]]), mel_padded)\n",
    "        attention_mask = torch.cat([torch.ones(mel_db.shape[:2], dtype=torch.int32), torch.zeros((mel_db.shape[0], num_pad + 1))], dim=1)\n",
    "\n",
    "        global_attention_mask = torch.zeros(input_embed.shape[:2], dtype=torch.int32)\n",
    "        global_attention_mask[:, 0] = 1\n",
    "\n",
    "        # load precomputed midi\n",
    "        midi_encoded = np.load(os.path.join(self.preprocess_folder, \"midi\", f\"{music_id}.{arranger_id}.midi.npy\"))\n",
    "        midi = torch.tensor(midi_encoded, dtype=torch.int32)\n",
    "        midi = torch.cat([torch.tensor([[self.tokenizer[\"BOS_None\"]]]), midi, torch.tensor([[self.tokenizer[\"EOS_None\"]]])], dim=-1)\n",
    "\n",
    "        # padding with -100\n",
    "        if midi.shape[1] < self.max_output_length:\n",
    "            midi = torch.cat([midi, torch.ones((midi.shape[0], self.max_output_length - midi.shape[1]), dtype=torch.int32) * -100], dim=-1)\n",
    "        else:\n",
    "            midi = midi[:, :self.max_output_length]\n",
    "\n",
    "        return {\"inputs_embeds\": input_embed.squeeze(0), \n",
    "                \"attention_mask\": attention_mask.squeeze(0), \n",
    "                #\"global_attention_mask\": global_attention_mask.squeeze(0), \n",
    "                \"labels\": midi.squeeze(0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5bf2bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257659/2731034332.py:14: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  remi_config = TokenizerConfig(**TOKENIZER_PARAMS)\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": (0, 127),\n",
    "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "    \"num_velocities\": 32,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "    \"use_chords\": True,\n",
    "    \"use_rests\": False,\n",
    "    \"use_tempos\": True,\n",
    "    \"use_time_signatures\": False,\n",
    "    \"use_programs\": False,\n",
    "    \"num_tempos\": 32,  # number of tempo bins\n",
    "    \"tempo_range\": (40, 250),  # (min, max)\n",
    "}\n",
    "remi_config = TokenizerConfig(**TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4af7d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_tokenizer = REMI(remi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a824404",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = REMI(remi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e924b74-7227-4913-9da3-110cf467272b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48423d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/miditok/midi_tokenizer.py:3252: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  self.config = TokenizerConfig()\n",
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/miditok/classes.py:702: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  return cls(**input_dict, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = REMI(params=\"song2midi/dataset/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6751f1a-0022-49ac-b688-c178f53d7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_paths = list(Path(\"song2midi\", \"dataset\", \"midi_blend\").glob(\"*.mid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa7fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(vocab_size=50265, files_paths=midi_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6fcb318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1169)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenizer(midi_paths[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef03f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e76718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_params(\"song2midi/dataset/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81e8dd1-ce38-4584-9060-74cc10ee6519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music id: -7AMMFFiGLU is too long skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music id: 7Y-XlFtao_A is too long skipping\n",
      "music id: bRBn5EY505E is too long skipping\n",
      "music id: C5tXGwUCD1c is too long skipping\n",
      "music id: cBC6c99OrOM is too long skipping\n",
      "music id: CPLK6L1fq7k is too long skipping\n",
      "music id: f3YhK-oZmY8 is too long skipping\n",
      "music id: gwOGlPU63Dw is too long skipping\n",
      "music id: HKtxkk3JvbQ is too long skipping\n",
      "music id: hTbeVXuWyaU is too long skipping\n",
      "music id: k0J1z82kVYQ is too long skipping\n",
      "music id: KpS-9OcYyqk is too long skipping\n",
      "music id: LEJlWRpqy2U is too long skipping\n",
      "music id: lzyl7abaPe0 is too long skipping\n",
      "music id: nFG3l5zxLdM is too long skipping\n",
      "music id: Pa4EGtUfDvI is too long skipping\n",
      "music id: r1k3hpTdc60 is too long skipping\n",
      "music id: RcQy61-7F7c is too long skipping\n",
      "music id: S8HAUbBLCPM is too long skipping\n",
      "music id: T4yW6TwjNLM is too long skipping\n",
      "music id: VnWo9-Dioik is too long skipping\n",
      "music id: XbZHt8b7YTY is too long skipping\n",
      "music id: XKROtTZs8iE is too long skipping\n",
      "music id: Y0ygWXF-5tI is too long skipping\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(tokenizer, music_folder=\"song2midi/dataset/song_blend\", midi_folder=\"song2midi/dataset/midi_blend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac148cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicDataset(tokenizer, ArrangerEmbedding(), max_output_length=4096, max_input_length=6144, mel_processing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eee934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in dataset:\n",
    "    if a[\"inputs_embeds\"].shape[0] != 6144 or a[\"attention_mask\"].shape[0] != 6144 or a[\"labels\"].shape[0] != 4096:\n",
    "        print(a[\"inputs_embeds\"].shape, a[\"attention_mask\"].shape, a[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5adbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3328b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b99489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da17ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reformer_pytorch import ReformerLM, Reformer\n",
    "from axial_positional_embedding import AxialPositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c386d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerEncoderDecoderConfig(PretrainedConfig):\n",
    "    def __init__(self,\n",
    "                  vocab_size=tokenizer.vocab_size, \n",
    "                  d_model=128,\n",
    "                  num_heads=8, \n",
    "                  encoder_layers=6, \n",
    "                  decoder_layers=6, \n",
    "                  encoder_max_seq_len=6144,\n",
    "                  decoder_max_seq_len=4096,\n",
    "                  encoder_axial_position_shape=(96, 64),\n",
    "                  decoder_axial_position_shape=(64, 64),\n",
    "                  pad_token_id=0,\n",
    "                  bos_token_id=1,\n",
    "                  eos_token_id=2,\n",
    "                  **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.encoder_max_seq_len = encoder_max_seq_len\n",
    "        self.decoder_max_seq_len = decoder_max_seq_len\n",
    "        self.encoder_axial_position_shape = encoder_axial_position_shape\n",
    "        self.decoder_axial_position_shape = decoder_axial_position_shape\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53617dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerEncoderDecoder(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "        self.bos_token_id = config.bos_token_id\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "\n",
    "        self.encoder = Reformer(\n",
    "            dim=config.d_model,\n",
    "            depth=config.encoder_layers,\n",
    "            heads=config.num_heads,\n",
    "        )\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim=config.d_model,\n",
    "            depth=config.decoder_layers,\n",
    "            heads=config.num_heads,\n",
    "            max_seq_len=config.decoder_max_seq_len,\n",
    "            num_tokens=config.vocab_size,\n",
    "            axial_position_emb=True,\n",
    "            axial_position_shape=config.decoder_axial_position_shape,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        self.position_embedding = AxialPositionalEmbedding(\n",
    "            config.d_model,\n",
    "            axial_shape=config.encoder_axial_position_shape\n",
    "        )\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    def pad_to_multiple(self, tensor, seq_len, multiple, dim=-1):\n",
    "        m = seq_len / multiple\n",
    "        if m.is_integer():\n",
    "            return tensor\n",
    "        \n",
    "        remainder = math.ceil(m) * multiple - seq_len\n",
    "        pad_offset = (0,) * (-1 - dim) * 2\n",
    "        return nn.functional.pad(tensor, (*pad_offset, 0, remainder), value=self.pad_token_id)\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    # pad_dim = -1 if its LM model else -2\n",
    "    def auto_paddding(self, input_ids, pad_dim, bucket_size, num_mem_kv, full_attn_thres, keys=None, input_mask=None, input_attn_mask=None):\n",
    "        device = input_ids.device\n",
    "\n",
    "        batch_size, t = input_ids.shape[:2]\n",
    "\n",
    "        keys_len = 0 if keys is None else keys.shape[1]\n",
    "        seq_len = t + num_mem_kv + keys_len\n",
    "        \n",
    "\n",
    "        if seq_len > full_attn_thres:\n",
    "            if input_mask is None:\n",
    "                input_mask = torch.full((batch_size, t), True, dtype=torch.bool, device=device)\n",
    "\n",
    "            input_ids = self.pad_to_multiple(input_ids, seq_len, bucket_size * 2, dim=pad_dim)\n",
    "\n",
    "            if input_mask is not None:\n",
    "                input_mask = nn.functional.pad(input_mask, (0, input_ids.shape[1] - input_mask.shape[1]), value=False)\n",
    "\n",
    "            if input_attn_mask is not None:\n",
    "                offset = input_ids.shape[1] - input_attn_mask.shape[1]\n",
    "                input_attn_mask = nn.functional.pad(input_attn_mask, (0, offset, 0, offset), value=False)\n",
    "\n",
    "        return input_ids, input_mask, input_attn_mask\n",
    "\n",
    "\n",
    "    def shift_tokens_right(self, input_ids):\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = self.eos_token_id\n",
    "\n",
    "        if self.pad_token_id is None:\n",
    "            raise ValueError(\"config.pad_token_id has to be defined.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, self.pad_token_id)\n",
    "\n",
    "        return shifted_input_ids\n",
    "\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None, decoder_input=None, labels=None):\n",
    "        if decoder_input is None:\n",
    "            decoder_input = self.shift_tokens_right(labels)\n",
    "\n",
    "        # encoder\n",
    "        encoder_input = inputs_embeds + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_output = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        decoder_output = self.decoder(decoder_input, keys=encoder_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(decoder_output.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            return {\"loss\": masked_lm_loss, \"logits\": decoder_output}\n",
    "        \n",
    "        return {\"logits\": decoder_output}\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, inputs_embeds, attention_mask=None, max_length=4096, temperature=1.0, top_k=50, top_p=1):\n",
    "        is_training = self.training\n",
    "        device = inputs_embeds.device\n",
    "\n",
    "        # padding settings\n",
    "        pad_dim = -1\n",
    "        bucket_size = self.decoder.reformer.bucket_size\n",
    "        num_mem_kv = self.decoder.reformer.num_mem_kv\n",
    "        full_attn_thres = self.decoder.reformer.full_attn_thres\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        # encoder\n",
    "        encoder_input = inputs_embeds + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_keys = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        generated = torch.tensor([self.bos_token_id], device=device).unsqueeze(0)\n",
    "\n",
    "        decoder_mask = torch.full_like(generated, True, dtype=torch.bool, device=device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            generated = generated[:, -self.config.decoder_max_seq_len:]\n",
    "            decoder_mask = decoder_mask[:, -self.config.decoder_max_seq_len:]\n",
    "\n",
    "            generated, decoder_mask, _ = self.auto_paddding(generated, \n",
    "                                                             pad_dim, \n",
    "                                                             bucket_size, \n",
    "                                                             num_mem_kv, \n",
    "                                                             full_attn_thres, \n",
    "                                                             keys=encoder_keys, \n",
    "                                                             input_mask=decoder_mask)\n",
    "            \n",
    "            logits = self.decoder(generated, input_mask=decoder_mask, keys=encoder_keys)[:, -1, :]  / temperature\n",
    "\n",
    "            if top_k > 0:\n",
    "                top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "                filtered_logits = torch.full_like(logits, -float('Inf'))\n",
    "                logits = filtered_logits.scatter(1, top_k_indices, top_k_values)\n",
    "\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "                sorted_logits[sorted_indices_to_remove] = -float('Inf')\n",
    "                logits = sorted_logits.scatter(1, sorted_indices, sorted_logits)\n",
    "\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "            if next_token == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "        self.train(is_training)\n",
    "        return generated\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a0098",
   "metadata": {},
   "source": [
    "## Absolute pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8edbf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device)\n",
    "        return self.emb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59be388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerEncoderDecoder(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "        self.bos_token_id = config.bos_token_id\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "\n",
    "        self.projection = nn.Linear(128, config.d_model, bias=False)\n",
    "\n",
    "        self.encoder = Reformer(\n",
    "            dim=config.d_model,\n",
    "            depth=config.encoder_layers,\n",
    "            heads=config.num_heads,\n",
    "        )\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim=config.d_model,\n",
    "            depth=config.decoder_layers,\n",
    "            heads=config.num_heads,\n",
    "            max_seq_len=config.decoder_max_seq_len,\n",
    "            num_tokens=config.vocab_size,\n",
    "            absolute_position_emb=True,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        self.position_embedding = AbsolutePositionalEmbedding(\n",
    "            config.d_model,\n",
    "            config.encoder_max_seq_len\n",
    "        )\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    def pad_to_multiple(self, tensor, seq_len, multiple, dim=-1):\n",
    "        m = seq_len / multiple\n",
    "        if m.is_integer():\n",
    "            return tensor\n",
    "        \n",
    "        remainder = math.ceil(m) * multiple - seq_len\n",
    "        pad_offset = (0,) * (-1 - dim) * 2\n",
    "        return nn.functional.pad(tensor, (*pad_offset, 0, remainder), value=self.pad_token_id)\n",
    "\n",
    "    # https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/autopadder.py\n",
    "    # pad_dim = -1 if its LM model else -2\n",
    "    def auto_paddding(self, input_ids, pad_dim, bucket_size, num_mem_kv, full_attn_thres, keys=None, input_mask=None, input_attn_mask=None):\n",
    "        device = input_ids.device\n",
    "\n",
    "        batch_size, t = input_ids.shape[:2]\n",
    "\n",
    "        keys_len = 0 if keys is None else keys.shape[1]\n",
    "        seq_len = t + num_mem_kv + keys_len\n",
    "        \n",
    "\n",
    "        if seq_len > full_attn_thres:\n",
    "            if input_mask is None:\n",
    "                input_mask = torch.full((batch_size, t), True, dtype=torch.bool, device=device)\n",
    "\n",
    "            input_ids = self.pad_to_multiple(input_ids, seq_len, bucket_size * 2, dim=pad_dim)\n",
    "\n",
    "            if input_mask is not None:\n",
    "                input_mask = nn.functional.pad(input_mask, (0, input_ids.shape[1] - input_mask.shape[1]), value=False)\n",
    "\n",
    "            if input_attn_mask is not None:\n",
    "                offset = input_ids.shape[1] - input_attn_mask.shape[1]\n",
    "                input_attn_mask = nn.functional.pad(input_attn_mask, (0, offset, 0, offset), value=False)\n",
    "\n",
    "        return input_ids, input_mask, input_attn_mask\n",
    "\n",
    "\n",
    "    def shift_tokens_right(self, input_ids):\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = self.eos_token_id\n",
    "\n",
    "        if self.pad_token_id is None:\n",
    "            raise ValueError(\"config.pad_token_id has to be defined.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, self.pad_token_id)\n",
    "\n",
    "        return shifted_input_ids\n",
    "\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None, decoder_input=None, labels=None):\n",
    "        if decoder_input is None:\n",
    "            decoder_input = self.shift_tokens_right(labels)\n",
    "\n",
    "        # encoder\n",
    "        projected_input = self.projection(inputs_embeds)\n",
    "\n",
    "        encoder_input = projected_input + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_output = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        decoder_output = self.decoder(decoder_input, keys=encoder_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(decoder_output.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            return {\"loss\": masked_lm_loss, \"logits\": decoder_output}\n",
    "        \n",
    "        return {\"logits\": decoder_output}\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, inputs_embeds, attention_mask=None, max_length=4096, temperature=1.0, top_k=50, top_p=1):\n",
    "        is_training = self.training\n",
    "        device = inputs_embeds.device\n",
    "\n",
    "        # padding settings\n",
    "        pad_dim = -1\n",
    "        bucket_size = self.decoder.reformer.bucket_size\n",
    "        num_mem_kv = self.decoder.reformer.num_mem_kv\n",
    "        full_attn_thres = self.decoder.reformer.full_attn_thres\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "        # encoder\n",
    "        projected_input = self.projection(inputs_embeds)\n",
    "\n",
    "        encoder_input = projected_input + self.position_embedding(inputs_embeds)\n",
    "\n",
    "        encoder_keys = self.encoder(encoder_input, input_mask=attention_mask.bool())\n",
    "\n",
    "        # decoder\n",
    "        generated = torch.tensor([self.bos_token_id], device=device).unsqueeze(0)\n",
    "\n",
    "        decoder_mask = torch.full_like(generated, True, dtype=torch.bool, device=device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            generated = generated[:, -self.config.decoder_max_seq_len:]\n",
    "            decoder_mask = decoder_mask[:, -self.config.decoder_max_seq_len:]\n",
    "\n",
    "            generated, decoder_mask, _ = self.auto_paddding(generated, \n",
    "                                                             pad_dim, \n",
    "                                                             bucket_size, \n",
    "                                                             num_mem_kv, \n",
    "                                                             full_attn_thres, \n",
    "                                                             keys=encoder_keys, \n",
    "                                                             input_mask=decoder_mask)\n",
    "            \n",
    "            logits = self.decoder(generated, input_mask=decoder_mask, keys=encoder_keys)[:, -1, :]  / temperature\n",
    "\n",
    "            if top_k > 0:\n",
    "                top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "                filtered_logits = torch.full_like(logits, -float('Inf'))\n",
    "                logits = filtered_logits.scatter(1, top_k_indices, top_k_values)\n",
    "\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                sorted_indices_to_remove[:, 0] = 0\n",
    "\n",
    "                sorted_logits[sorted_indices_to_remove] = -float('Inf')\n",
    "                logits = sorted_logits.scatter(1, sorted_indices, sorted_logits)\n",
    "\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "            if next_token == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "        self.train(is_training)\n",
    "        return generated\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348619f",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2155a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RedConfig = ReformerEncoderDecoderConfig()\n",
    "\n",
    "model = ReformerEncoderDecoder(RedConfig).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "37291c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.generate(torch.rand((1, 4096, 128)).cuda(), torch.ones((1, 4096)).cuda(), max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75ae50db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,  ..., 36353, 38423,  6528]], device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90165c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m model(inputs_embeds\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6144\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6144\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50265\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4096\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "a = model(inputs_embeds=torch.rand((1, 6144, 128)).cuda(), attention_mask=torch.zeros(1, 6144).bool().cuda(), labels=torch.randint(0, 50265, (1, 4096)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "065ae39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(10.9838, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'logits': tensor([[[-0.3129,  0.6523, -0.8083,  ..., -0.4176, -1.0049,  0.2438],\n",
       "          [ 0.0567,  0.5744,  0.1914,  ..., -0.9457, -0.1222, -0.0429],\n",
       "          [-0.7221, -0.7545, -0.3690,  ..., -0.1682,  0.3251,  0.0503],\n",
       "          ...,\n",
       "          [ 0.0889, -0.3190,  0.0809,  ..., -0.2278,  0.2671, -0.0050],\n",
       "          [ 0.3629, -0.9432,  0.6089,  ...,  0.3671,  0.0901,  0.0371],\n",
       "          [-0.2118, -0.7043, -0.0478,  ..., -0.7430,  0.5263,  0.3794]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "52451974",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = torch.argmax(a[\"logits\"], dim=-1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "a3e02cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5df85388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4, 228, 347, ...,  81, 147, 199])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(aaaaa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0d82f681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor(tokenizer.tokens_errors([np.array(aaaaa[0]), np.array(aaaaa[0])]))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fd83be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs_embeds': tensor([[   0.1421,    0.5948,    0.1657,  ...,    1.7305,   -0.4943,\n",
       "             0.1316],\n",
       "         [-100.0000, -100.0000, -100.0000,  ...,   -6.1386,   -2.3168,\n",
       "            -6.0267],\n",
       "         [  -6.4156,   -2.1213,   -0.1338,  ...,  -10.6941,   -6.8124,\n",
       "            -6.9586],\n",
       "         ...,\n",
       "         [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "             0.0000],\n",
       "         [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "             0.0000],\n",
       "         [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "             0.0000]], grad_fn=<SqueezeBackward1>),\n",
       " 'attention_mask': tensor([1., 1., 1.,  ..., 0., 0., 0.]),\n",
       " 'labels': tensor([   1,  612,    4,  ..., -100, -100, -100])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38c17e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model.generate(data[\"inputs_embeds\"].unsqueeze(0).cuda(), data[\"attention_mask\"].unsqueeze(0).cuda(), max_length=2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39bab98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(inputs_embeds=data[\"inputs_embeds\"].unsqueeze(0).cuda(), attention_mask=data[\"attention_mask\"].unsqueeze(0).cuda(), labels=data[\"labels\"].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61889c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17a58e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  612,    4,  ..., 4119, 4119, 4119]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out[\"logits\"].argmax(dim=-1).cpu()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f05a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = tokenizer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea479075",
   "metadata": {},
   "outputs": [],
   "source": [
    "export.dump_midi(\"song2midi/results/generated.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93b041ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"song2midi/aaaa/reformer_encoder_decoder_1_17.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb15b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c69623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71ab84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncoderDecoder(\n",
       "  (encoder): Reformer(\n",
       "    (layers): ReversibleSequence(\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x ReversibleBlock(\n",
       "          (f): Deterministic(\n",
       "            (net): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): LSHSelfAttention(\n",
       "                (toqk): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (tov): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): FullQKAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (local_attn): LocalAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (g): Deterministic(\n",
       "            (net): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Chunk(\n",
       "                (fn): FeedForward(\n",
       "                  (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (irrev_blocks): ModuleList(\n",
       "        (0-5): 6 x IrreversibleBlock(\n",
       "          (f): PreNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): LSHSelfAttention(\n",
       "              (toqk): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (tov): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (lsh_attn): LSHAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (full_attn): FullQKAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (local_attn): LocalAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (g): PreNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Chunk(\n",
       "              (fn): FeedForward(\n",
       "                (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ReformerLM(\n",
       "    (token_emb): Embedding(50265, 128)\n",
       "    (to_model_dim): Identity()\n",
       "    (pos_emb): AxialPositionalEmbedding()\n",
       "    (layer_pos_emb): Always()\n",
       "    (reformer): Reformer(\n",
       "      (layers): ReversibleSequence(\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x ReversibleBlock(\n",
       "            (f): Deterministic(\n",
       "              (net): PreNorm(\n",
       "                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=128, out_features=512, bias=False)\n",
       "                  (tov): Linear(in_features=128, out_features=512, bias=False)\n",
       "                  (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (g): Deterministic(\n",
       "              (net): PreNorm(\n",
       "                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (irrev_blocks): ModuleList(\n",
       "          (0-5): 6 x IrreversibleBlock(\n",
       "            (f): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): LSHSelfAttention(\n",
       "                (toqk): Linear(in_features=128, out_features=512, bias=False)\n",
       "                (tov): Linear(in_features=128, out_features=512, bias=False)\n",
       "                (to_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): FullQKAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (local_attn): LocalAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (g): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Chunk(\n",
       "                (fn): FeedForward(\n",
       "                  (w1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (w2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (out): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Linear(in_features=128, out_features=50265, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (position_embedding): AxialPositionalEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6354e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 1 loss: 3.001537322998047: 100%|| 200/200 [13:25<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1 avg_loss: 3.292593777179718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 loss: 2.7438926696777344 tok_error: 0.1187744140625: 100%|| 40/40 [00:21<00:00,  1.87it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 avg_loss: 3.214534121751785 avg_tok_error: 0.136553955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 2 loss: 3.1599314212799072: 100%|| 200/200 [13:14<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 2 avg_loss: 3.263686016201973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 2 loss: 2.758176326751709 tok_error: 0.09735107421875: 100%|| 40/40 [00:21<00:00,  1.90it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 2 avg_loss: 3.206604093313217 avg_tok_error: 0.116510009765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 3 loss: 3.634474992752075: 100%|| 200/200 [13:13<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 3 avg_loss: 3.2527723687887193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 3 loss: 2.7613515853881836 tok_error: 0.114013671875: 100%|| 40/40 [00:21<00:00,  1.89it/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 3 avg_loss: 3.1994731426239014 avg_tok_error: 0.13108978271484376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 4 loss: 3.4382355213165283: 100%|| 200/200 [12:39<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 4 avg_loss: 3.251042654514313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 4 loss: 2.770627975463867 tok_error: 0.1146240234375: 100%|| 40/40 [00:20<00:00,  1.98it/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 4 avg_loss: 3.1884994506835938 avg_tok_error: 0.13695068359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 5 loss: 2.9310972690582275: 100%|| 200/200 [12:38<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 5 avg_loss: 3.2381991171836852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation epoch: 5 loss: 2.7556653022766113 tok_error: 0.1142578125: 100%|| 40/40 [00:20<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 5 avg_loss: 3.179256671667099 avg_tok_error: 0.1314971923828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr: 0.0003125 training epoch: 6 loss: 3.547208786010742:  94%|| 189/200 [11:57<00:41,  3.79s/it]"
     ]
    }
   ],
   "source": [
    "dataloader_train = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "num_epochs = 100\n",
    " \n",
    "optimizer = AdamW(model.parameters(), lr=0.0003125)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=5, factor=0.3125, threshold=0.01)\n",
    "\n",
    "old_loss = 1000000\n",
    "time = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # training\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    progess_bar = tqdm(dataloader_train)\n",
    "    for batch in progess_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outp = model(\n",
    "            inputs_embeds=batch[\"inputs_embeds\"].cuda(),\n",
    "            attention_mask=batch['attention_mask'].to('cuda'),\n",
    "            labels=batch[\"labels\"].cuda()\n",
    "            )\n",
    "        \n",
    "        loss = outp[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        progess_bar.set_description(f\"lr: {scheduler.get_last_lr()[0]} training epoch: {epoch+1} loss: {loss.item()}\")\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(dataloader_train)\n",
    "    print(f\"training epoch: {epoch+1} avg_loss: {avg_train_loss}\")\n",
    "\n",
    "    # evaluation\n",
    "    val_loss = 0\n",
    "    tok_error = 0\n",
    "    model.eval()\n",
    "    progess_bar = tqdm(dataloader_val)\n",
    "    for batch in progess_bar:\n",
    "        with torch.no_grad():\n",
    "            outp = model(\n",
    "                inputs_embeds=batch[\"inputs_embeds\"].cuda(),\n",
    "                attention_mask=batch['attention_mask'].to('cuda'),\n",
    "                labels=batch[\"labels\"].cuda()\n",
    "                )\n",
    "            \n",
    "            loss = outp[\"loss\"]\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            tok_err = torch.mean(torch.tensor(tokenizer.tokens_errors(torch.argmax(outp[\"logits\"], dim=-1).cpu()))).item()\n",
    "            tok_error += tok_err\n",
    "\n",
    "            progess_bar.set_description(f\"validation epoch: {epoch+1} loss: {loss.item()} tok_error: {tok_err}\")\n",
    "\n",
    "    avg_val_loss = val_loss / len(dataloader_val)\n",
    "    avg_tok_error = tok_error / len(dataloader_val)\n",
    "    print(f\"validation epoch: {epoch+1} avg_loss: {avg_val_loss} avg_tok_error: {avg_tok_error}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < old_loss:\n",
    "        old_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"song2midi/check/reformer_encoder_decoder_{time}_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc8a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f684a6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDSeq2SeqLMOutput(loss=tensor(10.8519, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.1528,  0.0000,  1.7232,  ...,  0.0340, -0.2018,  0.2016],\n",
       "         [ 0.0276,  0.0000,  0.0030,  ..., -0.0170,  0.3117, -0.0377],\n",
       "         [ 0.1439,  0.0000, -0.2117,  ..., -0.1489, -0.0806,  0.2360],\n",
       "         ...,\n",
       "         [-0.0684,  0.0000,  0.4367,  ..., -0.0780,  0.1614, -0.0231],\n",
       "         [-0.1011,  0.0000,  0.0282,  ...,  0.4681,  0.0164,  0.3435],\n",
       "         [-0.4156,  0.0000, -0.0677,  ...,  0.2259, -0.0516, -0.0998]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.0419,  0.3264, -0.3593,  ...,  0.9347, -0.8379, -1.3963],\n",
       "         [-0.3056,  0.7967, -0.5429,  ...,  1.9927,  0.8124,  0.7674],\n",
       "         [-0.5460,  0.8657, -1.1845,  ..., -1.8351,  0.3854,  0.0470],\n",
       "         ...,\n",
       "         [ 2.3461,  2.7531,  2.2392,  ..., -0.1812, -0.4449, -0.2723],\n",
       "         [ 0.9922, -2.1367, -2.2770,  ..., -0.0409,  2.7945,  0.0236],\n",
       "         [-1.1363, -0.6906, -0.8586,  ..., -2.3521, -2.1718, -1.1611]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None, encoder_global_attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "460474f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 18:55:23.024028: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-28 18:55:23.093224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 18:55:23.710833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d73a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"song2midi/results\",\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=5,\n",
    "    log_level=\"passive\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    save_safetensors=False,\n",
    "    num_train_epochs=500,\n",
    "    learning_rate=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e17b669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3d43ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='371' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 371/5000 21:21 < 4:28:00, 0.29 it/s, Epoch 37/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>10.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>10.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>10.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>10.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>10.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>9.897300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>9.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>9.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>9.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>9.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>9.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>9.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>9.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>9.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>9.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>9.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>9.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>9.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>9.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>8.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>8.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>8.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>8.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>8.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>8.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>8.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>8.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>8.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>8.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>8.744300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>8.729400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/transformers/trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/accelerate/accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2125\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c3aef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "miditok.utils.filter_dataset(Path(\"song2midi\", \"dataset\", \"midi\").glob(\"*.mid\"), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6173afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symusic import Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a5c0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = Score(\"song2midi/dataset/midi/This_Game.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1fddbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "miditok.utils.merge_same_program_tracks(aaa.tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5d69a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(ttype=Tick, tpq=480, begin=0, end=138155, tracks=1, notes=1488, time_sig=1, key_sig=4, markers=0, lyrics=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c420860",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(\"song2midi/dataset/wav/Idol.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0e603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4978177,), 22050)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d826b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = librosa.feature.melspectrogram(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc3245e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 9724)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10cd3a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-440246"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4978177 - 5418423"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
