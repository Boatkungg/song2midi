{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8277daf3-e0d9-4efa-9058-4b4e67d41572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import miditok\n",
    "from miditok import REMI, TokenizerConfig\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from symusic import Score\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ba3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ded318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0081b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 22:16:31.159800: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-06 22:16:31.187197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 22:16:31.809371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import SEWModel, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a832c7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dd79c6c26c4c4bae87e1c3bf907645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9e4b0d46264bd19ec62d7b50d17e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec982691f6e47b68a012f308e8a205f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9315ebefb64e1ebf2b9f66bb10dc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6533cbc4024a7290372df83ef5638d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517781c37839435eba44d5f62ebd77e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/163M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"asapp/sew-tiny-100k-ft-ls100h\")\n",
    "model = SEWModel.from_pretrained(\"asapp/sew-tiny-100k-ft-ls100h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ca0a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEWModel(\n",
       "  (feature_extractor): SEWFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): SEWGroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 64, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(64, 128, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (2): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (3): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(128, 128, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (4): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(128, 256, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (6): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (7): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (8): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (9): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(256, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (10): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (11): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (12): SEWNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (feature_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (encoder): SEWEncoder(\n",
       "    (pos_conv_embed): SEWPositionalConvEmbedding(\n",
       "      (conv): Conv1d(512, 512, kernel_size=(31,), stride=(2,), padding=(15,), groups=16)\n",
       "      (padding): SEWSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SEWEncoderLayer(\n",
       "        (attention): SEWAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): SEWFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (upsample): SEWUpsampling(\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ab2184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = torchaudio.load(\"song2midi/dataset/song/anima__full.mp3\")\n",
    "audio = torch.mean(audio, dim=0)\n",
    "audio = torchaudio.transforms.Resample(sr, 16000)(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6b8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b5fc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4274838])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9813489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs[\"input_values\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed50094a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13358, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "271abae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs = encodec_model.encode(inputs[\"input_values\"].cuda(), inputs[\"padding_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f90e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 20039])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.audio_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5252f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MambaConfig, MambaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdae2af",
   "metadata": {},
   "source": [
    "![dataset](images/Dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0469a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [name.split(\".\")[0] for name in os.listdir(\"song2midi/dataset/song\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82620d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [name.split(\".\")[:2] for name in os.listdir(\"song2midi/dataset/midi\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5dc52ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '1'],\n",
       " ['10', '1'],\n",
       " ['2', '1'],\n",
       " ['3', '1'],\n",
       " ['4', '1'],\n",
       " ['5', '1'],\n",
       " ['6', '1'],\n",
       " ['7', '1'],\n",
       " ['8', '1'],\n",
       " ['9', '1']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[midi for midi in b if midi[0] in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23e4cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 129, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.rand((1, 128, 1024)), torch.rand((1, 1, 1024))], dim=-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3541e196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) + torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2dfe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f5cb8b9d5c42918107d4a8616641fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426ccc4452b74e89b1852f34e82c71eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77fa0732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[12092,    13,   619,  4370,   310, 20295]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c54e8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaEmbedding(nn.Module):\n",
    "  def __init__(self, vocab_size, arranger_ids=256, hidden_size=128):\n",
    "    super().__init__()\n",
    "    self.arranger_embeddings = nn.Embedding(arranger_ids, hidden_size)\n",
    "    self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "  def forward(self, arranger_id, mel_db, token):\n",
    "    return torch.cat([self.arranger_embeddings(arranger_id), mel_db, self.token_embeddings(token)], dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43204a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, tokenizer, embbeding, max_output_length=4096, music_folder=\"song2midi/dataset/song\", midi_folder=\"song2midi/dataset/midi\", compressor=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embbeding = embbeding\n",
    "        self.max_output_length = max_output_length\n",
    "        self.music_folder = music_folder\n",
    "        self.midi_folder = midi_folder\n",
    "        \n",
    "        self.music_ids = [name.split(\".\")[0] for name in os.listdir(music_folder)]\n",
    "        \n",
    "        uncheck_midis = [name.split(\".\")[:2] for name in os.listdir(midi_folder)]\n",
    "        self.midis = [midi for midi in uncheck_midis if midi[0] in self.music_ids] # [music_id, arranger_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.music_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        music_id, arranger_id = self.midis[idx]\n",
    "\n",
    "        # music\n",
    "        music_path = os.path.join(self.music_folder, f\"{music_id}.mp3\")\n",
    "        waveform, sr = torchaudio.load(music_path)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr)\n",
    "        mel = mel_transform(waveform)\n",
    "        mel_db = torchaudio.transforms.AmplitudeToDB()(mel)\n",
    "\n",
    "        mel_scaled = torch.nn.functional.interpolate(mel_db.unsqueeze(1), size=(128, 1023), mode='bilinear', align_corners=False).squeeze(1)\n",
    "        mel_shape = mel_scaled.shape\n",
    "        mel_scaled = mel_scaled.reshape(mel_shape[0], mel_shape[2], mel_shape[1])\n",
    "\n",
    "        input_embed = self.embbeding(torch.tensor([[int(arranger_id)]]), mel_scaled)\n",
    "        attention_mask = torch.ones(input_embed.shape[:2], dtype=torch.int32)\n",
    "\n",
    "        global_attention_mask = torch.zeros(input_embed.shape[:2], dtype=torch.int32)\n",
    "        global_attention_mask[:, 0] = 1\n",
    "\n",
    "        # midi\n",
    "        midi_path = os.path.join(self.midi_folder, f\"{music_id}.{arranger_id}.mid\")\n",
    "        score = Score(midi_path)\n",
    "\n",
    "        miditok.utils.merge_same_program_tracks(score.tracks)\n",
    "\n",
    "        midi = torch.tensor(self.tokenizer.encode(score), dtype=torch.int32)\n",
    "        # print(torch.tensor([self.tokenizer[\"BOS_None\"]]).shape, midi.shape, torch.tensor([self.tokenizer[\"EOS_None\"]]).shape)\n",
    "        midi = torch.cat([torch.tensor([[self.tokenizer[\"BOS_None\"]]]), midi, torch.tensor([[self.tokenizer[\"EOS_None\"]]])], dim=-1)\n",
    "\n",
    "        # padding with -100\n",
    "        if midi.shape[0] < self.max_output_length:\n",
    "            midi = torch.cat([midi, torch.ones((midi.shape[0], self.max_output_length - midi.shape[1]), dtype=torch.int32) * -100], dim=-1)\n",
    "        else:\n",
    "            midi = midi[:, :self.max_output_length]\n",
    "\n",
    "        return {\"inputs_embeds\": input_embed.squeeze(0), \"attention_mask\": attention_mask.squeeze(0), \"global_attention_mask\": global_attention_mask.squeeze(0), \"labels\": midi.squeeze(0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3574a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tokenizer, sample_rate=22050, music_folder=\"song2midi/dataset/song\", midi_folder=\"song2midi/dataset/midi\", output_folder=\"song2midi/dataset/preprocess\"):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_folder, \"mel\")):\n",
    "        os.makedirs(os.path.join(output_folder, \"mel\"))\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_folder, \"midi\")):\n",
    "        os.makedirs(os.path.join(output_folder, \"midi\"))\n",
    "                    \n",
    "    music_ids = [name.split(\".\")[0] for name in os.listdir(music_folder)]\n",
    "    uncheck_midis = [name.split(\".\")[:2] for name in os.listdir(midi_folder)]\n",
    "    midis = [midi for midi in uncheck_midis if midi[0] in music_ids] # [music_id, arranger_id]\n",
    "\n",
    "    skipped_music = []\n",
    "\n",
    "    for music_id in music_ids:\n",
    "        music_path = os.path.join(music_folder, f\"{music_id}.mp3\")\n",
    "        waveform, sr = librosa.load(music_path, sr=sample_rate)\n",
    "\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=2048, hop_length=1536, n_mels=128)\n",
    "        mel = mel_transform(torch.tensor(waveform).unsqueeze(0))\n",
    "        mel_db = torchaudio.transforms.AmplitudeToDB()(mel)\n",
    "\n",
    "        mel_shape = mel_db.shape\n",
    "        if mel_shape[2] > 4096: # have to skip this music\n",
    "            print(f\"music id: {music_id} is too long skipping\")\n",
    "            skipped_music.append(music_id)\n",
    "            continue\n",
    "\n",
    "        mel_db = mel_db.reshape(mel_shape[0], mel_shape[2], mel_shape[1])\n",
    "\n",
    "        np.save(os.path.join(output_folder, \"mel\", f\"{music_id}.mel.npy\"), mel_db.numpy())\n",
    "\n",
    "    for music_id, arranger_id in midis:\n",
    "        if music_id in skipped_music:\n",
    "            continue\n",
    "        \n",
    "        midi_path = os.path.join(midi_folder, f\"{music_id}.{arranger_id}.mid\")\n",
    "        score = Score(midi_path)\n",
    "        miditok.utils.merge_same_program_tracks(score.tracks)\n",
    "        midi_encoded = tokenizer.encode(score)\n",
    "        np.save(os.path.join(output_folder, \"midi\", f\"{music_id}.{arranger_id}.midi.npy\"), midi_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a46acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_interpolate(mel):\n",
    "    mel_shape = mel.shape[1]\n",
    "    return nn.functional.interpolate(mel.unsqueeze(1), size=(math.floor(mel_shape / 4), 128), mode='bilinear', align_corners=False).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "674b8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, tokenizer, embbeding, max_output_length=4096, max_input_length=1024, mel_processing=None, preprocess_folder=\"song2midi/dataset/preprocess\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embbeding = embbeding\n",
    "        self.max_output_length = max_output_length\n",
    "        self.max_input_length = max_input_length\n",
    "        self.preprocess_folder = preprocess_folder\n",
    "        self.mel_processing = mel_processing\n",
    "        \n",
    "        self.data_files = [name.split(\".\")[:2] for name in os.listdir(os.path.join(preprocess_folder, \"midi\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        music_id, arranger_id = self.data_files[idx]\n",
    "\n",
    "        # load precomputed mel'\n",
    "        mel_db = np.load(os.path.join(self.preprocess_folder, \"mel\", f\"{music_id}.mel.npy\"))\n",
    "        mel_db = torch.tensor(mel_db)\n",
    "\n",
    "        if self.mel_processing:\n",
    "            mel_db = self.mel_processing(mel_db)\n",
    "\n",
    "        if mel_db.shape[1] < self.max_input_length - 1:\n",
    "            num_pad = self.max_input_length - mel_db.shape[1] - 1\n",
    "            mel_padded = torch.cat([mel_db, torch.zeros((1, num_pad, mel_db.shape[2]))], dim=1)\n",
    "        else:\n",
    "            num_pad = 1\n",
    "            mel_padded = mel_db[:, :self.max_input_length - 1]\n",
    "\n",
    "        input_embed = self.embbeding(torch.tensor([[int(arranger_id)]]), mel_padded)\n",
    "        attention_mask = torch.cat([torch.ones(mel_db.shape[:2], dtype=torch.int32), torch.zeros((mel_db.shape[0], num_pad + 1))], dim=1)\n",
    "\n",
    "        global_attention_mask = torch.zeros(input_embed.shape[:2], dtype=torch.int32)\n",
    "        global_attention_mask[:, 0] = 1\n",
    "\n",
    "        # load precomputed midi\n",
    "        midi_encoded = np.load(os.path.join(self.preprocess_folder, \"midi\", f\"{music_id}.{arranger_id}.midi.npy\"))\n",
    "        midi = torch.tensor(midi_encoded, dtype=torch.int32)\n",
    "        midi = torch.cat([torch.tensor([[self.tokenizer[\"BOS_None\"]]]), midi, torch.tensor([[self.tokenizer[\"EOS_None\"]]])], dim=-1)\n",
    "\n",
    "        # padding with -100\n",
    "        if midi.shape[0] < self.max_output_length:\n",
    "            midi = torch.cat([midi, torch.ones((midi.shape[0], self.max_output_length - midi.shape[1]), dtype=torch.int32) * -100], dim=-1)\n",
    "        else:\n",
    "            midi = midi[:, :self.max_output_length]\n",
    "\n",
    "        return {\"inputs_embeds\": input_embed.squeeze(0), \n",
    "                \"attention_mask\": attention_mask.squeeze(0), \n",
    "                #\"global_attention_mask\": global_attention_mask.squeeze(0), \n",
    "                \"labels\": midi.squeeze(0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b224c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, tokenizer, embbeding, max_output_length=4096, max_input_length=4096, mel_processing=None, preprocess_folder=\"song2midi/dataset/preprocess\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embbeding = embbeding\n",
    "        self.max_output_length = max_output_length\n",
    "        self.max_input_length = max_input_length\n",
    "        self.preprocess_folder = preprocess_folder\n",
    "        self.mel_processing = mel_processing\n",
    "        \n",
    "        self.data_files = [name.split(\".\")[:2] for name in os.listdir(os.path.join(preprocess_folder, \"midi\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        music_id, arranger_id = self.data_files[idx]\n",
    "\n",
    "        # load precomputed mel'\n",
    "        mel_db = np.load(os.path.join(self.preprocess_folder, \"mel\", f\"{music_id}.mel.npy\"))\n",
    "        mel_db = torch.tensor(mel_db)\n",
    "\n",
    "        if self.mel_processing:\n",
    "            mel_db = self.mel_processing(mel_db)\n",
    "\n",
    "        if mel_db.shape[1] > self.max_input_length - 1:\n",
    "            mel_db = mel_db[:, :self.max_input_length - 1]\n",
    "\n",
    "        pad_mel_db = nn.functional.pad(mel_db, (0, 0, 0, self.max_input_length - mel_db.shape[1]), value=0)\n",
    "\n",
    "        midi_encoded = np.load(os.path.join(self.preprocess_folder, \"midi\", f\"{music_id}.{arranger_id}.midi.npy\"))\n",
    "        midi = torch.tensor(midi_encoded, dtype=torch.int32)\n",
    "        midi = torch.cat([torch.tensor([[self.tokenizer[\"BOS_None\"]]]), midi, torch.tensor([[self.tokenizer[\"EOS_None\"]]])], dim=-1)\n",
    "\n",
    "        pad_midi = nn.functional.pad(midi, (0, self.max_output_length - midi.shape[1]), value=self.tokenizer[\"PAD_None\"])\n",
    "\n",
    "        pad_midi_labels = nn.functional.pad(midi, (0, self.max_output_length - midi.shape[1]), value=-100)\n",
    "\n",
    "        # print(pad_mel_db)\n",
    "        # print(pad_midi.shape)\n",
    "        # print(pad_midi_labels.shape)\n",
    "\n",
    "        input_embed = self.embbeding(torch.tensor([[int(arranger_id)]]), pad_mel_db, pad_midi)\n",
    "\n",
    "        labels = torch.cat([torch.ones((1, 1), dtype=torch.int32) * -100, torch.ones((pad_mel_db.shape[:2]), dtype=torch.int32) * -100, pad_midi_labels], dim=-1)\n",
    "\n",
    "\n",
    "        return {\"inputs_embeds\": input_embed.squeeze(0), \n",
    "                \"labels\": labels.squeeze(0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5bf2bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126903/2731034332.py:14: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  remi_config = TokenizerConfig(**TOKENIZER_PARAMS)\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": (0, 127),\n",
    "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "    \"num_velocities\": 32,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "    \"use_chords\": True,\n",
    "    \"use_rests\": False,\n",
    "    \"use_tempos\": True,\n",
    "    \"use_time_signatures\": False,\n",
    "    \"use_programs\": False,\n",
    "    \"num_tempos\": 32,  # number of tempo bins\n",
    "    \"tempo_range\": (40, 250),  # (min, max)\n",
    "}\n",
    "remi_config = TokenizerConfig(**TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a824404",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = REMI(remi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e924b74-7227-4913-9da3-110cf467272b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48423d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/miditok/midi_tokenizer.py:3252: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  self.config = TokenizerConfig()\n",
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/miditok/classes.py:702: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  return cls(**input_dict, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = REMI(params=\"song2midi/dataset/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6751f1a-0022-49ac-b688-c178f53d7fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa7fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "midi_paths = list(Path(\"song2midi\", \"dataset\", \"midi\").glob(\"*.mid\"))\n",
    "\n",
    "tokenizer.train(vocab_size=50265, files_paths=midi_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e76718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_params(\"song2midi/dataset/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f81e8dd1-ce38-4584-9060-74cc10ee6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad948a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(tokenizer, music_folder=\"song2midi/dataset/song_eval\", midi_folder=\"song2midi/dataset/midi_eval\", output_folder=\"song2midi/dataset/preprocess_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ac148cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicDataset(tokenizer, MambaEmbedding(tokenizer.vocab_size), max_output_length=4096, max_input_length=4096, mel_processing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1bc589a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = MusicDataset(tokenizer, MambaEmbedding(tokenizer.vocab_size), max_output_length=4096, max_input_length=4096, mel_processing=None, preprocess_folder=\"song2midi/dataset/preprocess_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eee934d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-100.0000,  -57.9684,  -56.6364,  ...,  -12.9613,  -12.0673,\n",
      "           -10.0208],\n",
      "         [ -19.8567,  -12.8137,  -16.1480,  ...,   -4.3473,   -1.0316,\n",
      "            -0.6634],\n",
      "         [  -4.0322,   -0.6607,    2.2929,  ...,   -7.8962,   -9.6513,\n",
      "            -4.9240],\n",
      "         ...,\n",
      "         [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000]]])\n",
      "torch.Size([1, 4096])\n",
      "torch.Size([1, 4096])\n",
      "torch.Size([8193, 128]) torch.Size([8193])\n"
     ]
    }
   ],
   "source": [
    "for a in dataset:\n",
    "    print(a[\"inputs_embeds\"].shape, a[\"labels\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d600fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_config = MambaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    pad_token_id=tokenizer[\"PAD_None\"],\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "\n",
    "model = MambaForCausalLM(mamba_config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12978488",
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = model(inputs_embeds=torch.rand((1, 4096, 128)).cuda(), labels=torch.randint(0, 50265, (1, 4096)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec8d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = model(inputs_embeds=dataset[0][\"inputs_embeds\"].unsqueeze(0).cuda(), labels=dataset[0][\"labels\"].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba23c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d5aa4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/15 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "dataloader_train = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "# dataloader_eval = DataLoader(eval_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(dataloader_train) * num_epochs))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # training\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        outp = model(inputs_embeds=batch[\"inputs_embeds\"].cuda(), labels=batch[\"labels\"].cuda())\n",
    "        loss = outp.loss\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(dataloader_train)\n",
    "    print(f\"epoch {epoch} train loss: {avg_train_loss}\")\n",
    "\n",
    "    # # evaluation\n",
    "    # eval_loss = 0\n",
    "    # model.eval()\n",
    "    # for batch in tqdm(dataloader_eval):\n",
    "    #     with torch.no_grad():\n",
    "    #         outp = model(inputs_embeds=batch[\"inputs_embeds\"].cuda(), labels=batch[\"labels\"].cuda())\n",
    "    #         loss = outp.loss\n",
    "\n",
    "    #         eval_loss += loss.item()\n",
    "\n",
    "    # avg_eval_loss = eval_loss / len(dataloader_eval)\n",
    "    # print(f\"epoch {epoch} eval loss: {avg_eval_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56c69623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6354e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk/miniconda3/envs/keras/lib/python3.11/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1/500: 100%|███████████████████████████████████████████████████| 9/9 [00:35<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.99042797088623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/500: 100%|███████████████████████████████████████████████████| 9/9 [00:35<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.955936643812391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 2/500: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 10.92811393737793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/500: 100%|███████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.921842045254177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/500: 100%|███████████████████████████████████████████████████| 9/9 [00:35<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.878047307332357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 4/500: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 10.814319610595703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/500: 100%|███████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.8076753616333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/500: 100%|███████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.721735636393229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 6/500: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 10.632669448852539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/500: 100%|███████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.612842241923014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/500: 100%|███████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.494063589307997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 8/500: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 10.405158996582031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/500: 100%|███████████████████████████████████████████████████| 9/9 [00:35<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.37434270646837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/500: 100%|██████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.261999660068089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 10/500: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 10.163861274719238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/500: 100%|██████████████████████████████████████████████████| 9/9 [00:34<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.145961231655544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/500: 100%|██████████████████████████████████████████████████| 9/9 [00:35<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 10.043963326348198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Epoch 12/500: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 9.93946647644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/500:  22%|███████████                                       | 2/9 [00:13<00:47,  6.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 30\u001b[0m\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     24\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m     25\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     26\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "val_dataloader = DataLoader(eval_dataset, batch_size=1)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "num_training_steps = len(train_dataloader) * 500\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training Loop\n",
    "num_train_epochs = 500\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_train_epochs}\"):\n",
    "        inputs_embeds = batch['inputs_embeds'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        \n",
    "        outputs = model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Evaluating Epoch {epoch+1}/{num_train_epochs}\"):\n",
    "                inputs_embeds = batch['inputs_embeds'].to('cuda')\n",
    "                attention_mask = batch['attention_mask'].to('cuda')\n",
    "                labels = batch['labels'].to('cuda')\n",
    "                \n",
    "                outputs = model(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs[\"loss\"]\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                eval_steps += 1\n",
    "        \n",
    "        avg_eval_loss = eval_loss / eval_steps\n",
    "        print(f\"Average Evaluation Loss: {avg_eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc8a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f684a6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDSeq2SeqLMOutput(loss=tensor(10.8519, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.1528,  0.0000,  1.7232,  ...,  0.0340, -0.2018,  0.2016],\n",
       "         [ 0.0276,  0.0000,  0.0030,  ..., -0.0170,  0.3117, -0.0377],\n",
       "         [ 0.1439,  0.0000, -0.2117,  ..., -0.1489, -0.0806,  0.2360],\n",
       "         ...,\n",
       "         [-0.0684,  0.0000,  0.4367,  ..., -0.0780,  0.1614, -0.0231],\n",
       "         [-0.1011,  0.0000,  0.0282,  ...,  0.4681,  0.0164,  0.3435],\n",
       "         [-0.4156,  0.0000, -0.0677,  ...,  0.2259, -0.0516, -0.0998]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.0419,  0.3264, -0.3593,  ...,  0.9347, -0.8379, -1.3963],\n",
       "         [-0.3056,  0.7967, -0.5429,  ...,  1.9927,  0.8124,  0.7674],\n",
       "         [-0.5460,  0.8657, -1.1845,  ..., -1.8351,  0.3854,  0.0470],\n",
       "         ...,\n",
       "         [ 2.3461,  2.7531,  2.2392,  ..., -0.1812, -0.4449, -0.2723],\n",
       "         [ 0.9922, -2.1367, -2.2770,  ..., -0.0409,  2.7945,  0.0236],\n",
       "         [-1.1363, -0.6906, -0.8586,  ..., -2.3521, -2.1718, -1.1611]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None, encoder_global_attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "460474f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 18:55:23.024028: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-28 18:55:23.093224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 18:55:23.710833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d73a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"song2midi/results\",\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=5,\n",
    "    log_level=\"passive\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    save_safetensors=False,\n",
    "    num_train_epochs=500,\n",
    "    learning_rate=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e17b669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3d43ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='371' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 371/5000 21:21 < 4:28:00, 0.29 it/s, Epoch 37/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>10.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>10.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>10.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>10.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>10.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>9.897300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>9.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>9.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>9.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>9.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>9.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>9.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>9.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>9.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>9.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>9.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>9.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>9.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>9.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>8.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>8.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>8.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>8.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>8.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>8.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>8.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>8.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>8.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>8.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>8.744300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>8.729400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/transformers/trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/accelerate/accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2125\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c3aef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "miditok.utils.filter_dataset(Path(\"song2midi\", \"dataset\", \"midi\").glob(\"*.mid\"), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6173afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symusic import Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a5c0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = Score(\"song2midi/dataset/midi/This_Game.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1fddbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "miditok.utils.merge_same_program_tracks(aaa.tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5d69a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(ttype=Tick, tpq=480, begin=0, end=138155, tracks=1, notes=1488, time_sig=1, key_sig=4, markers=0, lyrics=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c420860",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(\"song2midi/dataset/wav/Idol.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0e603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4978177,), 22050)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d826b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = librosa.feature.melspectrogram(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc3245e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 9724)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10cd3a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-440246"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4978177 - 5418423"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
